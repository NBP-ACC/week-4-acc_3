{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabrück University - A&C: Computational Cognition (Summer Term 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 02: Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in at 14:00 at **Tuesday, April 30, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your tutors. Please push your results to your Github group folder.\n",
    "\n",
    "In this exercise sheet you will have to work with ```pandas``` and ```seaborn```. ```pandas``` is one of the most preferred and widely used tools in data processing. What’s cool about ```pandas``` is that it takes data (like a CSV or TSV file, or a SQL database) and creates a Python object with rows and columns called 'data frame' that looks very similar to tables in a statistical software (think Excel or SPSS for example). ```pandas``` makes data processing a lot easier in comparison to working with lists and/or dictionaries through for-loops or list comprehension.  \n",
    "```seaborn``` is a library for making plots. It is based on ```matplotlib``` but offers more functions speicialized for statistical visualization. Also most people agree that ```seaborn``` looks more legit.\n",
    "\n",
    "Don't forget that you we will also give **2 points** for nice coding style!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Peer review for sheet 01 [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning this week you will have to make a peer review of the other groups' solutions. Each group reviews the solutions of two other groups and give points according to the given point distribution considering the correctness of the solution. For this reviews the tutors will give you up to 3 points each week.\n",
    "\n",
    "| * |Group 1|Group 2|Group 3|Group 4|Group 5|Group 6|Group 7|Group 8|Group 9|Group 10|Group 11|\n",
    "| ------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ | ------ |\n",
    "| check solutions of group: | 10, 7 | 4, 9  | 1, 4  | 11, 1 | 8, 11 | 5, 3  | 9, 10 | 6, 5  | 3, 2  | 2, 8   | 7, 6   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should open an issue in repositories of groups you have to check. The title of the issue should be your group name (e.g.\"Group 1\"). Comments on what was good and bad, how much points they get etc.  \n",
    "Refer to https://guides.github.com/features/issues/ to learn more about issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Dataframes [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matplotlib``` and ```seaborn``` should already be installed in your environment. If not please run:\n",
    "```sh\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Importing a csv file [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the csv files of all subjects into one dataframe. Make sure that each row has a unique index. You might want to take a look at what ***pandas.concat*** does.<br>\n",
    "Extra fun: Display the output of the dataframe using the ***pandas.set_option*** function to display the data in a well-arranged way. Play a little bit around with the settings that you are allowed to change.<br>\n",
    "Save ```df_concatenated```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For renaming our data, only execute once!\n",
    "# PATH = os.getcwd()+ \"/Data\"\n",
    "# all_files = glob.glob(os.path.join(PATH, \"*.csv\")) \n",
    "\n",
    "# dataframes = [pd.read_csv(fp, header=0) for fp in all_files]\n",
    "\n",
    "# for i, df in enumerate(dataframes):\n",
    "#     new_name = \"300{}\".format(i)\n",
    "#     df[\"SubjectID\"] = new_name\n",
    "#     df.to_csv(\"Data/\"+new_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For renaming the data of all groups, only execute once!\n",
    "# PATH = os.getcwd()+ \"/Data/all/\"\n",
    "\n",
    "# for group in range(1, 12):\n",
    "#     group_files = glob.glob(os.path.join(PATH+'Group_{}/'.format(group), \"*.csv\")) \n",
    "# #     print(group_files)\n",
    "#     group_dataframes = [pd.read_csv(filepath, header=0) for filepath in group_files]\n",
    "#     for subject_i, df in enumerate(group_dataframes):\n",
    "#         new_name = \"{:02}00{:02}\".format(group, subject_i+1)\n",
    "#         print(new_name)\n",
    "#         df[\"SubjectID\"] = new_name\n",
    "#         df.to_csv(os.path.join(PATH+'/new/{}.csv'.format(new_name)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SubjectID  StimulusType  response   RT\n",
      "0         30001             1         1  427\n",
      "1         30001             1         1  289\n",
      "2         30001             1         1  267\n",
      "3         30001             1         1  408\n",
      "4         30001             1         1  388\n",
      "5         30001             1         1  369\n",
      "6         30001             0         0    0\n",
      "7         30001             0         0    0\n",
      "8         30001             1         1  420\n",
      "9         30001             1         1  389\n",
      "...         ...           ...       ...  ...\n",
      "7190      40001             1         1  356\n",
      "7191      40001             1         1  272\n",
      "7192      40001             1         1  373\n",
      "7193      40001             1         1  340\n",
      "7194      40001             1         1  357\n",
      "7195      40001             1         1  305\n",
      "7196      40001             1         1  373\n",
      "7197      40001             1         1  339\n",
      "7198      40001             1         1  357\n",
      "7199      40001             1         1  356\n",
      "\n",
      "[7200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "PATH = os.getcwd()+ \"/Data\"\n",
    "all_files = glob.glob(os.path.join(PATH, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "                                                       # http://www.oipapio.com/question-88634\n",
    "    \n",
    "dataframes = [pd.read_csv(fp, header=0) for fp in all_files]\n",
    "\n",
    "df_concatenated = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "print(df_concatenated)\n",
    "\n",
    "df_concatenated\n",
    "\n",
    "# save concatenated dataframe\n",
    "OUTPUT_DIR = os.getcwd() + '/Processed/'\n",
    "DATAPATH_CONCAT = OUTPUT_DIR + 'data_concatenated_jan.csv'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "df_concatenated.to_csv(DATAPATH_CONCAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Working with dataframes [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add a column called \"congruence\" to ```df_concatenated```. The column should have a value *True* if \"StimulusType\" and \"reponse\" matches. Otherwise the column should have a value *False*.\n",
    "\n",
    "- Create a new dataframe which has \"SubjectID\",\"StiumulusType\",\"RT\" and \"congruence\" as a column. For each combination of \"SubjectID\" and \"StimulusType\" (e.g. \"7001\" and \"0\") compute the average RT and congruence level.\n",
    "\n",
    "- When computing the average RT, omit all reaction times which are 0 as these will manipulate the mean.\n",
    "\n",
    "- Rename \"congruence\" as \"accuracy\" and save the dataframe as a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                RT  congruence\n",
      "SubjectID StimulusType                        \n",
      "10001     0             258.000000      0.8000\n",
      "          1             282.354430      0.9875\n",
      "10002     0             293.500000      0.9000\n",
      "          1             349.487500      1.0000\n",
      "10003     0             269.600000      0.7500\n",
      "          1             380.562500      1.0000\n",
      "10004     0             372.500000      0.9000\n",
      "          1             377.650000      1.0000\n",
      "10005     0             265.333333      0.8500\n",
      "          1             342.362500      1.0000\n",
      "...                            ...         ...\n",
      "100004    0             308.500000      0.9000\n",
      "          1             360.775000      1.0000\n",
      "100005    0             305.500000      0.9000\n",
      "          1             343.175000      1.0000\n",
      "100006    0             241.666667      0.8500\n",
      "          1             341.300000      1.0000\n",
      "100007    0             278.500000      0.8000\n",
      "          1             305.450000      1.0000\n",
      "100008    0             295.200000      0.7500\n",
      "          1             355.662500      1.0000\n",
      "\n",
      "[132 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# add a column \"congruence\"\n",
    "# TODO\n",
    "df_concatenated = pd.read_csv(DATAPATH_CONCAT)\n",
    "# https://stackoverflow.com/questions/12376863/adding-calculated-columns-to-a-dataframe-in-pandas\n",
    "# The notation below gives the columns as Series, compares the two and sets the result as a new column 'congruence'\n",
    "df_concatenated['congruence'] = df_concatenated['StimulusType'] == df_concatenated['response']\n",
    "\n",
    "df_concatenated_avg = pd.DataFrame()\n",
    "\n",
    "df_concatenated_avg['RT'] = (df_concatenated[df_concatenated['RT']!=0].groupby(['SubjectID', 'StimulusType'])['RT'].mean())\n",
    "df_concatenated_avg['congruence'] = df_concatenated.groupby(['SubjectID', 'StimulusType'])['congruence'].mean()\n",
    "\n",
    "df_concatenated_avg.rename(columns={'congruence': 'accuracy'})\n",
    "\n",
    "# save averaged dataframe\n",
    "DATAPATH_CONCAT_AVG = os.getcwd() + '/Processed/data_concatenated_averaged_jan.csv'\n",
    "df_concatenated_avg.to_csv(DATAPATH_CONCAT_AVG)\n",
    "\n",
    "print(df_concatenated_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Statistical plotting [6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Boxplot and Violinplot [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the RT of each trial for all subjects as a stripplot and a boxplot on top of each other. Do the same with a striplot and a violinplot. Plot go trials as green dots and no-go trails as red dots. Reminder: don't forget to mask the data where RT=0. Make sure that the legends are informative (Don't display duplicated legends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "data_concat = pd.read_csv(os.getcwd() + \"/Processed/data_concatenated.csv\")\n",
    "\n",
    "# create two axes\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
    "\n",
    "# first subplot with stripplot and boxplot\n",
    "# TODO \n",
    "\n",
    "# second subplot with stripplot and violinplot\n",
    "# TODO\n",
    "\n",
    "# handling legends\n",
    "# TODO\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Violinplot combining all data of all groups [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a dataframe consisting of all data across groups. You already did this in 1.a). At the end this dataframe you should have 8 * 11 * 100 rows.\n",
    "\n",
    "- Every group has used their ID convention. Make sure that every data point follows this SubjectID system: group number + \"00\" + subject number.  \n",
    "e.g) 3002 for the second subject of the third group.\n",
    "\n",
    "- Compute average RT and accuaracy for each subject in the big dataframe you just created. You already did this in 1.b). At the end this dataframe will have 8 * 11 rows.\n",
    "\n",
    "- On the first column plot average RT and accuracy for 8 subjects from your group's data. Use violinplot and split go/no-go conditions.\n",
    "\n",
    "- On the second column plot average RT and accuracy for 80 subjects from all data. Use violinplot and split go/no-go conditions.\n",
    "\n",
    "- Do you see any difference between the first column and the second column? What does this tell us about the central limit theorem (CLT) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again create a concatenated dataframe over all (averaged) groups.\n",
    "# Don't forget to modify the Subject ID\n",
    "# TODO\n",
    "\n",
    "# Now it's time to plot your results\n",
    "figs, axes = plt.subplots(nrows=2, ncols=2, sharey=\"row\")\n",
    "\n",
    "# violin plot for your group's data\n",
    "# TODO\n",
    "\n",
    "# violin plot of all group's data\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two datasets and relate it with CLT. Write your opinion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Scatterplot [1 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatterplot comparing RT and accuracy. Do you see some correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
